apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: example-transformer
  namespace: default
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment    # 使用原始部署模式
    serving.kserve.io/skip-transformer-args: "true"     # 跳过 transformer 参数注入
spec:
  predictor:
    containers:
      - name: kserve-container
        image: nginx:alpine  # 使用轻量级的nginx镜像替代
        ports:
          - containerPort: 80  # 添加端口配置
        volumeMounts:
          - name: model-storage
            mountPath: "/mnt/models"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
    volumes:
      - name: model-storage
        hostPath:
          path: "/data/models"
          type: DirectoryOrCreate
  transformer:
    containers:
      - name: kserve-container
        image: nginx:alpine
        ports:
          - containerPort: 80
        command: ["/bin/sh"]
        args: 
          - "-c"
          - |
            /docker-entrypoint.sh nginx -g 'daemon off;' &    # 后台运行 nginx
            while true; do sleep 86400; done                  # 保持容器运行
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
    volumes:
      - name: transformer-config
        hostPath:
          path: "/data/transformer-config"
          type: DirectoryOrCreate